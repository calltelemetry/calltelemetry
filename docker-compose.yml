services:
  caddy:
    image: caddy:2-alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile
      - ./certs:/certs:ro
    networks:
      - ct
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "wget -q --spider http://127.0.0.1:80 || wget -q --spider http://127.0.0.1:2019/metrics",
        ]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 30s

  vue-web:
    image: "calltelemetry/vue:${VUE_VERSION:-0.8.6-rc47}"
    restart: "always"
    expose:
      - "80"
    networks:
      - ct
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://127.0.0.1:80"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 30s

  traceroute:
    image: "calltelemetry/traceroute:${TRACEROUTE_VERSION:-0.8.5-rc27}"
    user: root
    expose:
      - "4100"
    networks:
      - ct
    healthcheck:
      test: ["CMD-SHELL", "bash -c 'exec 3<>/dev/tcp/127.0.0.1/4100'"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 30s

  db:
    image: "calltelemetry/postgres:14"
    user: root
    restart: "always"
    shm_size: "8gb"
    environment:
      - POSTGRES_USER=${POSTGRES_USER:-calltelemetry}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-postgres}
      - POSTGRES_DB=${POSTGRES_DB:-calltelemetry_prod}
      - PGDATA=/bitnami/postgresql/data
    command: >
      postgres
      -c max_connections=300
    expose:
      - "5432"
    volumes:
      - ./postgres-data:/bitnami/postgresql
    networks:
      - ct
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $${POSTGRES_USER:-calltelemetry} -d $${POSTGRES_DB:-calltelemetry_prod}"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 30s

  web:
    image: "calltelemetry/web:${WEB_VERSION:-0.8.6-rc47}"
    restart: "always"
    user: root
    expose:
      - 4000
      - 4080
    ports:
      - "22:3022"
    environment:
      - DB_USER=${POSTGRES_USER:-calltelemetry}
      - DB_PASSWORD=${POSTGRES_PASSWORD:-postgres}
      - DB_HOSTNAME=db
      - DB_NAME=${POSTGRES_DB:-calltelemetry_prod}
      - DB_PORT=5432
      - EXTERNAL_IP=${DEFAULT_IPV4:-127.0.0.1}
      - LOGGING_LEVEL=warning
      - ADMIN_NODE=TRUE
      - WORKER_NODE=TRUE
      - CERT_KEY=/home/app/cert/appliance_key.pem
      - CERT_PUBLIC=/home/app/cert/appliance.crt
      # can be HACKNEY, IBROWSE, MINT, GUN or HTTPC.
      - HTTP_ADAPTER=HACKNEY
      - LOG_PATH=/var/log
      - ERL_CRASH_DUMP=/tmp/crash-dumps/erl_crash.dump
      - ERL_CRASH_DUMP_NICE=1
      - PROM_EX_UPLOAD_DASHBOARDS=${PROM_EX_UPLOAD_DASHBOARDS:-false}
      - GRAFANA_HOST=${GRAFANA_HOST_INTERNAL:-http://grafana:3000/grafana}
      - GRAFANA_TOKEN=${GRAFANA_TOKEN:-}
      # JTAPI / phone control (populated by ct-cli or cli.sh when JTAPI is enabled)
      - JTAPI_MODE=${JTAPI_MODE:-disabled}
      - JTAPI_SIDECAR_ENDPOINT=${JTAPI_SIDECAR_ENDPOINT:-}
      - JTAPI_SIDECAR_URL=${JTAPI_SIDECAR_URL:-}
      - S3_ENABLED=${S3_ENABLED:-false}
      - S3_ENDPOINT=${S3_ENDPOINT:-http://seaweedfs:8333}
      - S3_ACCESS_KEY_ID=${S3_ACCESS_KEY_ID:-minioadmin}
      - S3_SECRET_ACCESS_KEY=${S3_SECRET_ACCESS_KEY:-minioadmin}
      - S3_REGION=us-east-1
      - CT_MEDIA_ENDPOINT=${CT_MEDIA_ENDPOINT:-}
      - DOCKER_SOCKET=/var/run/docker.sock
      - NATS_URL=nats://nats:4222
      # OpenTelemetry (enable via: ct otel enable)
      - OTEL_ENABLED=${OTEL_ENABLED:-false}
      - OTEL_EXPORTER_OTLP_ENDPOINT=${OTEL_EXPORTER_OTLP_ENDPOINT:-http://otel-collector:4318}
    tmpfs:
      - /var/log:rw,mode=0555,size=5000m
    networks:
      - ct
    volumes:
      - ./certs:/home/app/cert:rw
      - ./crash-dumps:/tmp/crash-dumps:rw
      # Persistent org-scoped media storage (JTAPI audio, recordings, etc)
      - ./org-data:/data:rw
      - /var/run/docker.sock:/var/run/docker.sock
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "7" # Keeps 7 days of logs
        compress: "true" # Compresses rotated files
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://127.0.0.1:4080/healthz || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 30s

  nats:
    image: "nats:2.11"
    command: "-c /etc/nats/nats.conf"
    volumes:
      - ./nats.conf:/etc/nats/nats.conf
      - nats-data:/jetstream/store
    expose:
      - "4222"
    networks:
      - ct

  prometheus:
    image: prom/prometheus:v2.52.0
    restart: "always"
    user: root
    ports:
      - "9090:9090"
    command:
      - --config.file=/etc/prometheus/prometheus.yml
      - --storage.tsdb.path=/prometheus
      - --storage.tsdb.retention.time=15d
      - --web.console.libraries=/usr/share/prometheus/console_libraries
      - --web.console.templates=/usr/share/prometheus/consoles
      - --web.external-url=/prometheus
      - --web.route-prefix=/prometheus
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    networks:
      - ct

  grafana:
    image: grafana/grafana:12.2.1
    restart: "always"
    user: "472"
    depends_on:
      - prometheus
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=${GF_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GF_ADMIN_PASSWORD:-calltelemetry}
      - GF_PATHS_PROVISIONING=/etc/grafana/provisioning
      - GF_DASHBOARDS_DEFAULT_HOME_DASHBOARD_PATH=/var/lib/grafana/dashboards/calltelemetry-overview.json
      - GF_SERVER_ROOT_URL=/grafana
      - GF_SERVER_SERVE_FROM_SUB_PATH=true
    volumes:
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
      - ./grafana/dashboards:/var/lib/grafana/dashboards:ro
      - grafana-data:/var/lib/grafana
    networks:
      - ct

  # ─────────────────────────────────────────────────────────────────
  # OpenTelemetry Collector (activate with COMPOSE_PROFILES=otel)
  # Receives OTLP from web + sidecar, exports metrics to Prometheus.
  # Enable via: ct otel enable  (or COMPOSE_PROFILES=otel)
  # ─────────────────────────────────────────────────────────────────

  otel-collector:
    profiles: ["otel"]
    image: otel/opentelemetry-collector-contrib:0.96.0
    restart: unless-stopped
    command: ["--config=/etc/otelcol/config.yaml"]
    volumes:
      - ./otel-collector/otel-collector-config.yaml:/etc/otelcol/config.yaml:ro
    expose:
      - "4317"   # OTLP gRPC
      - "4318"   # OTLP HTTP
      - "8889"   # Prometheus metrics
    networks:
      - ct

  # ─────────────────────────────────────────────────────────────────
  # JTAPI Phone Control Stack (activate with COMPOSE_PROFILES=jtapi)
  # ─────────────────────────────────────────────────────────────────

  jtapi-jar-init:
    profiles: ["jtapi"]
    image: natsio/nats-box:0.14.5
    pull_policy: if_not_present
    restart: "no"
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "Checking NATS ObjectStore for JTAPI JAR..."
        BUCKET="jtapi-jars-$${NATS_ORG_ID:-1}"
        # Wait for NATS to be ready
        for i in $$(seq 1 30); do
          nats -s "$${NATS_URL}" server ping --count=1 2>/dev/null && break
          echo "  Waiting for NATS... ($$i/30)"
          sleep 2
        done
        # Try to download JAR from ObjectStore
        if nats -s "$${NATS_URL}" object get "$$BUCKET" jtapi.jar --output /jars/jtapi.jar 2>/dev/null; then
          SIZE=$$(stat -c%s /jars/jtapi.jar 2>/dev/null || echo "unknown")
          echo "Downloaded jtapi.jar from NATS ObjectStore ($$SIZE bytes)"
        elif [ -f /jars/jtapi.jar ]; then
          echo "JAR already present on volume, skipping download"
        else
          echo "No JAR in NATS ObjectStore yet — sidecar will wait for upload via UI"
        fi
    environment:
      - NATS_URL=nats://nats:4222
      - NATS_ORG_ID=1
    volumes:
      - jtapi-jars:/jars
    depends_on:
      nats:
        condition: service_started
    networks:
      - ct

  jtapi-sidecar:
    profiles: ["jtapi"]
    image: "calltelemetry/jtapi-sidecar:${JTAPI_VERSION:-1.14.5}"
    restart: unless-stopped
    mem_limit: 1g
    expose:
      - "50051"
      - "8080"
    environment:
      - SPRING_PROFILES_ACTIVE=standalone
      - NATS_URL=nats://nats:4222
      - NATS_ORG_ID=1
      - POD_NAME=jtapi-sidecar-1
      - GRPC_PORT=50051
      - JTAPI_PORT=8080
      # Auto-connect/configure via NATS KV (CtiConfigWatcher)
      - CTI_POOL_AUTO_CONNECT=true
      - CTI_POOL_AUTO_CONFIGURE=true
      - CTI_POOL_SOURCE=nats
      - CTI_POOL_ENABLE_MEDIA=true
      # S3 object storage for audio
      - S3_ENABLED=true
      - S3_ENDPOINT=http://seaweedfs:8333
      - S3_ACCESS_KEY_ID=${S3_ACCESS_KEY_ID:-minioadmin}
      - S3_SECRET_ACCESS_KEY=${S3_SECRET_ACCESS_KEY:-minioadmin}
      # JAR auto-restart when downloaded from NATS ObjectStore
      - WATCH_JARS=true
      # OpenTelemetry (enable via: ct otel enable)
      - OTEL_ENABLED=${OTEL_ENABLED:-false}
      - OTEL_SDK_DISABLED=${OTEL_SDK_DISABLED:-true}
      - OTEL_TRACES_EXPORTER=${OTEL_TRACES_EXPORTER:-none}
      - OTEL_METRICS_EXPORTER=${OTEL_METRICS_EXPORTER:-none}
      - OTEL_EXPORTER_OTLP_ENDPOINT=${OTEL_EXPORTER_OTLP_ENDPOINT:-http://otel-collector:4317}
      # Disable Spring Boot tracing autoconfiguration when OTel is off
      - MANAGEMENT_TRACING_ENABLED=${OTEL_ENABLED:-false}
    volumes:
      - jtapi-jars:/app/jars
    depends_on:
      jtapi-jar-init:
        condition: service_completed_successfully
      nats:
        condition: service_started
    networks:
      - ct
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://127.0.0.1:8080/actuator/health || curl -sf http://127.0.0.1:8080/actuator/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 90s

  ct-media:
    profiles: ["jtapi"]
    image: "calltelemetry/ct-media:${CT_MEDIA_VERSION:-0.2.5}"
    restart: unless-stopped
    expose:
      - "50053"
    environment:
      - GRPC_PORT=50053
      - S3_ENDPOINT=seaweedfs:8333
      - S3_ACCESS_KEY_ID=${S3_ACCESS_KEY_ID:-minioadmin}
      - S3_SECRET_ACCESS_KEY=${S3_SECRET_ACCESS_KEY:-minioadmin}
      # OpenTelemetry (enable via: ct otel enable)
      - OTEL_ENABLED=${OTEL_ENABLED:-false}
      - OTEL_EXPORTER_OTLP_ENDPOINT=${OTEL_EXPORTER_OTLP_ENDPOINT:-http://otel-collector:4317}
      - OTEL_SERVICE_NAME=ct-media
    networks:
      - ct
    healthcheck:
      test: ["CMD", "bash", "-c", "echo > /dev/tcp/127.0.0.1/50053"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    depends_on:
      seaweedfs:
        condition: service_healthy

  seaweedfs:
    profiles: ["jtapi"]
    image: chrislusf/seaweedfs:4.13
    pull_policy: if_not_present
    command: >
      server -s3 -s3.port=8333 -s3.config=/etc/seaweedfs/s3.json
      -master.volumeSizeLimitMB=100
    restart: unless-stopped
    expose:
      - "8333"   # S3 API
      - "8888"   # Filer
      - "9333"   # Master
    volumes:
      - seaweedfs-data:/data
      - type: bind
        source: ./seaweedfs-s3.json
        target: /etc/seaweedfs/s3.json
        read_only: true
    networks:
      - ct
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://127.0.0.1:9333/cluster/healthz || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s

networks:
  ct:
    # enable_ipv6: true

volumes:
  certs:
    driver: local
  crash-dumps:
    driver: local
  traefik-logs:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local
  nats-data:
    driver: local
  jtapi-jars:
    driver: local
  seaweedfs-data:
    driver: local
